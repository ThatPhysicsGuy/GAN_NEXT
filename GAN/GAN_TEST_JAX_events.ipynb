{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8564726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os, sys\n",
    "import pathlib\n",
    "import jax\n",
    "import pandas as pd\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jax.example_libraries import stax\n",
    "from jax import grad, jit, vmap\n",
    "import time\n",
    "\n",
    "dir_dir = '/Users/mxd6118/Desktop/DiffSim/'\n",
    "#src_dir = os.path.dirname(dir_dir) + \"/src/\"\n",
    "sys.path.insert(0,'/Users/mxd6118/Desktop/DiffSim')\n",
    "from Plots import *\n",
    "\n",
    "from src.simulator.NEW_Simulator_normal import simulate_waveforms, init_params\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a73149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, name_data):\n",
    "        \n",
    "        with open(f'{name_data}.pickle','wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "            f.close()\n",
    "\n",
    "class Producer():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.key = random.PRNGKey(int(time.time()))\n",
    "        number_of_events = int(input('How many events to be produced?'))\n",
    "        params_path = dir_dir + \"/bin/output/8530/krypton/test_noise_constant_uniform/trained_params.pickle\"\n",
    "        params = self.load_state(params_path)\n",
    "        \n",
    "        \n",
    "        self.dataloader  = self.build_dataloader(number_of_events)\n",
    "        \n",
    "        \n",
    "        batch_real = next(self.dataloader.iterate())\n",
    "        batch_real['Label'] = np.ones(len( batch_real['energy_deposits']))\n",
    "        \n",
    "        energy_depo = batch_real['energy_deposits']#self.build_random_batch(number_of_events)\n",
    "        \n",
    "        start = time.time()\n",
    "        produced_pmt,produced_sipm = self.arrays(energy_depo,params)\n",
    "        time_taken = time.time() - start\n",
    "        \n",
    "        batch_fake = {\"energy_deposits\": energy_depo,\n",
    "                 \"PMT_FAKE\": np.array(produced_pmt),\n",
    "                 \"SIPM_FAKE\": np.array(produced_sipm),\n",
    "                 \"Label_tempo\": np.zeros(len(energy_depo))}\n",
    "        \n",
    "        self.data_set = batch_real | batch_fake\n",
    "        \n",
    "        #get_data(data_set,f'batch_produced_{number_of_events}')\n",
    "        \n",
    "        print(f'All done, time taken {time_taken} sec')\n",
    "\n",
    "    def build_dataloader(self,number_of_events):\n",
    "\n",
    "        from src.utils.dataloaders.krypton_DATES_CUSTOM_DROPOUT import krypton\n",
    "        # Load the sipm database:\n",
    "        sipm_db = pd.read_pickle(\"/Users/mxd6118/Desktop/DiffSim/database/new_sipm.pkl\")\n",
    "    \n",
    "        dl = krypton(\n",
    "            batch_size  = number_of_events,\n",
    "            db          = sipm_db,\n",
    "            path        = \"/Users/mxd6118/Desktop/DiffSim/kdst\",\n",
    "            run         = 8530,\n",
    "            shuffle = True,\n",
    "            drop = 0,\n",
    "            z_slice = 0,\n",
    "            )\n",
    "            \n",
    "        return dl\n",
    "\n",
    "    \n",
    "    def arrays(self,monitor_data,params):\n",
    "        \n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        # First, run the monitor data through the simulator:\n",
    "        simulated_pmts, simulated_sipms = simulate_waveforms(monitor_data, params, subkey)\n",
    "        \n",
    "        return simulated_pmts, simulated_sipms\n",
    "        \n",
    "    def load_state(self,file):\n",
    "        with open(file,\"rb\") as f:\n",
    "            params = pickle.load(f)\n",
    "        return params\n",
    "        \n",
    "\n",
    "    def build_random_batch(self, number_of_events):\n",
    "    \n",
    "        batch =[]\n",
    "        for i in range(0,number_of_events):\n",
    "            one = np.hstack((np.random.uniform(low = -150, high = 150),\n",
    "                             np.random.uniform(low = -150, high = 150),\n",
    "                             np.random.uniform(low = 20,   high = 500),0.0415575))\n",
    "\n",
    "            two = np.vstack((one,np.zeros(4)))\n",
    "    \n",
    "            batch.append(two)\n",
    "\n",
    "        return np.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adc42d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.example_libraries import optimizers as jax_opt\n",
    "\n",
    "class GAN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        prod = Producer()\n",
    "        \n",
    "        self.data = prod.data_set\n",
    "        \n",
    "        self.batch = self.Chanteclair(self.data)\n",
    "        \n",
    "        print(self.batch['Train'].shape,flush = True)\n",
    "        \n",
    "        self.key = random.PRNGKey(int(time.time()))\n",
    "        \n",
    "        self.key, self.subkey = random.split(self.key)\n",
    "        \n",
    "        parameters, self.dis_apply = self.init_params(self.subkey)\n",
    "        \n",
    "        #print(self.out_size, flush = True)\n",
    "        \n",
    "        self.trainer = self.build_trainer(self.batch, self.dis_apply, parameters)\n",
    "        \n",
    "       \n",
    "    def Chanteclair(self,data):\n",
    "        train_batch_filtered = {}\n",
    "        train_batch_filtered['S2Si'] = []\n",
    "        train_batch_filtered['SIPM_FAKE'] =[]\n",
    "\n",
    "\n",
    "        for n in range(0,len(data['energy_deposits'])):\n",
    "            train_batch_filtered['S2Si'].append(data['S2Si'][n])\n",
    "            train_batch_filtered['SIPM_FAKE'].append(data['SIPM_FAKE'][n])\n",
    "\n",
    "\n",
    "        l = len(train_batch_filtered['S2Si'])\n",
    "\n",
    "        train_batch_filtered['train'] = np.vstack((train_batch_filtered['S2Si'],\n",
    "                                                   train_batch_filtered['SIPM_FAKE']))\n",
    "        \n",
    "        labels =[]\n",
    "\n",
    "        for c in range(0,2*l):\n",
    "            if c < l:\n",
    "                labels.append(np.array((1,0)))\n",
    "            else:\n",
    "                labels.append(np.array((0,1)))\n",
    "\n",
    "        train_batch_filtered['Labels'] = np.array(labels)\n",
    "\n",
    "\n",
    "        train, labels = sklearn.utils.shuffle(train_batch_filtered['train'],\n",
    "                                              train_batch_filtered['Labels'])\n",
    "\n",
    "\n",
    "        batch = {'Train': train, 'Labels' :labels}\n",
    "        \n",
    "        return batch \n",
    "        \n",
    "    \n",
    "    def init_params(self, subkey):\n",
    "        \n",
    "        dis_init, dis_apply = stax.serial(\n",
    "            stax.Flatten,\n",
    "            stax.Dense(128),stax.Sigmoid,\n",
    "            stax.Dense(16), stax.Sigmoid,\n",
    "            stax.Dense(2),stax.Softmax\n",
    "        )\n",
    "        \n",
    "        dis_out_size, dis_network_params = dis_init(subkey,(1,47,47,550))\n",
    "        \n",
    "        parameters = {\n",
    "        'D_parameters': dis_network_params\n",
    "        }\n",
    "        \n",
    "        return parameters, dis_apply\n",
    "    \n",
    "    def build_trainer(self, batch, fn, params):\n",
    "\n",
    "        # Shouldn't reach this portion unless training.\n",
    "        trainer = GAN_trainer(batch, fn, params)\n",
    "        \n",
    "        return trainer\n",
    "\n",
    "   \n",
    "    def train(self):\n",
    "        c = 0\n",
    "        self.key = jax.random.PRNGKey(int(time.time()))\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "\n",
    "        while c <= 100:\n",
    "\n",
    "            metrics = {}\n",
    "            start = time.time()\n",
    "\n",
    "            metrics[\"io_time\"] = time.time() - start\n",
    "\n",
    "            train_metrics, opt_state, acc = self.trainer.train_iteration(self.batch, c)\n",
    "            \n",
    "            #print('train metrics',train_metrics.keys(),flush = True)\n",
    "\n",
    "            # print(model_parameters.keys())\n",
    "            # print(model_parameters['diffusion'])\n",
    "            \n",
    "            metrics.update(train_metrics)\n",
    "\n",
    "            metrics['time'] = time.time() - start\n",
    "            metrics['accuracy'] = acc\n",
    "        \n",
    "\n",
    "            if c % 1 == 0:\n",
    "                print(f\"step = {c}, loss = {metrics['loss/loss']:.3f}, acc = {metrics['accuracy']:.3f}, time = {metrics['time']:.3f}\",flush = True)\n",
    "\n",
    "            c += 1\n",
    "            \n",
    "        #get_data(self.trainer.get_params(opt_state),'D_params')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1eb37d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-8 # Small value to avoid division by zero\n",
    "    y_pred = jnp.clip(y_pred, epsilon, 1.0 - epsilon)  # Clip values to prevent NaNs\n",
    "    loss = -(y_true * jnp.log(y_pred) + (1 - y_true) * jnp.log(1 - y_pred))\n",
    "    return jnp.mean(loss)\n",
    "    \n",
    "\n",
    "class GAN_trainer():\n",
    "    \n",
    "    def __init__(self,batch,fn,parameters):\n",
    "        \n",
    "        self.key = jax.random.PRNGKey(int(time.time()))\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        self.dis_apply = fn\n",
    "        \n",
    "        @jit\n",
    "        def forward_pass(batch, parameters, key):\n",
    "\n",
    "            fake_labels = self.dis_apply(parameters['D_parameters'], batch['Train'])\n",
    "        \n",
    "            loss = binary_cross_entropy(batch['Labels'],fake_labels)\n",
    "                          \n",
    "            return loss\n",
    "        \n",
    "        self.gradient_fn = jit(jax.value_and_grad(forward_pass, argnums=1))\n",
    "        \n",
    "        opt_init, opt_update, get_params = jax_opt.adamax(2e-3)\n",
    "            \n",
    "            \n",
    "        self.opt_state = opt_init(parameters)\n",
    "\n",
    "        self.opt_update = opt_update\n",
    "        self.get_params = get_params\n",
    "    \n",
    "    def parameters(self):\n",
    "        \n",
    "        p = self.get_params(self.opt_state)\n",
    "        \n",
    "        #print(p.keys())\n",
    "    \n",
    "        parameters =  p['D_parameters']\n",
    "        \n",
    "        return parameters\n",
    "        \n",
    "    \n",
    "    def train_iteration(self, batch, c):\n",
    "        \n",
    "        metrics = {}\n",
    "\n",
    "        self.parameters = self.get_params(self.opt_state)\n",
    "        \n",
    "        #print(parameters)\n",
    "\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "\n",
    "        loss, gradients = self.gradient_fn(batch, self.parameters, subkey)\n",
    "        \n",
    "        #print(gradients['Dis_parameters'])\n",
    "        \n",
    "        self.opt_state = self.opt_update(c, gradients, self.opt_state)\n",
    "\n",
    "        metrics['loss/loss'] = loss\n",
    "\n",
    "        metrics.update(self.parameters)\n",
    "        \n",
    "        accuracy = self.acc(self.parameters,self.dis_apply, batch)\n",
    "\n",
    "        return metrics, self.opt_state, accuracy\n",
    "    \n",
    "    def acc(self, parameters,fn,batch):\n",
    "        \n",
    "        fake_labels = fn(parameters['D_parameters'], batch['Train'])\n",
    "        \n",
    "        accuracy = len(np.unique(np.where(np.argmax(fake_labels, axis = 1) - np.argmax(batch['Labels'],axis=1) == 0))) /len(batch['Labels'])\n",
    "        \n",
    "        return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7d3d65c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many events to be produced?10\n",
      "All done, time taken 1.2766032218933105 sec\n",
      "(20, 47, 47, 550)\n",
      "step = 0, loss = 0.706, acc = 0.500, time = 1.915\n",
      "step = 1, loss = 0.433, acc = 0.950, time = 1.195\n",
      "step = 2, loss = 0.295, acc = 0.950, time = 1.312\n",
      "step = 3, loss = 0.220, acc = 1.000, time = 1.287\n",
      "step = 4, loss = 0.177, acc = 1.000, time = 1.242\n",
      "step = 5, loss = 0.150, acc = 1.000, time = 1.234\n",
      "step = 6, loss = 0.132, acc = 1.000, time = 1.243\n",
      "step = 7, loss = 0.118, acc = 1.000, time = 1.271\n",
      "step = 8, loss = 0.105, acc = 1.000, time = 1.221\n",
      "step = 9, loss = 0.095, acc = 1.000, time = 1.239\n",
      "step = 10, loss = 0.087, acc = 1.000, time = 1.238\n",
      "step = 11, loss = 0.080, acc = 1.000, time = 1.249\n",
      "step = 12, loss = 0.074, acc = 1.000, time = 1.199\n",
      "step = 13, loss = 0.070, acc = 1.000, time = 1.233\n",
      "step = 14, loss = 0.066, acc = 1.000, time = 1.263\n",
      "step = 15, loss = 0.062, acc = 1.000, time = 1.293\n",
      "step = 16, loss = 0.059, acc = 1.000, time = 1.207\n",
      "step = 17, loss = 0.056, acc = 1.000, time = 1.261\n",
      "step = 18, loss = 0.054, acc = 1.000, time = 1.456\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mGAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 99\u001b[0m, in \u001b[0;36mGAN.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     97\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m---> 99\u001b[0m train_metrics, opt_state, acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m#print('train metrics',train_metrics.keys(),flush = True)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# print(model_parameters.keys())\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# print(model_parameters['diffusion'])\u001b[39;00m\n\u001b[1;32m    106\u001b[0m metrics\u001b[38;5;241m.\u001b[39mupdate(train_metrics)\n",
      "Cell \u001b[0;32mIn[31], line 60\u001b[0m, in \u001b[0;36mGAN_trainer.train_iteration\u001b[0;34m(self, batch, c)\u001b[0m\n\u001b[1;32m     56\u001b[0m loss, gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_fn(batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters, subkey)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#print(gradients['Dis_parameters'])\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss/loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     64\u001b[0m metrics\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/jax/example_libraries/optimizers.py:197\u001b[0m, in \u001b[0;36moptimizer.<locals>.tree_opt_maker.<locals>.tree_update\u001b[0;34m(i, grad_tree, opt_state)\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(tree, tree2))\n\u001b[1;32m    196\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(tree_unflatten, subtrees, states_flat)\n\u001b[0;32m--> 197\u001b[0m new_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m new_states_flat, subtrees2 \u001b[38;5;241m=\u001b[39m unzip2(\u001b[38;5;28mmap\u001b[39m(tree_flatten, new_states))\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subtree, subtree2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(subtrees, subtrees2):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/jax/example_libraries/optimizers.py:452\u001b[0m, in \u001b[0;36madamax.<locals>.update\u001b[0;34m(i, g, state)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(i, g, state):\n\u001b[1;32m    451\u001b[0m   x, m, u \u001b[38;5;241m=\u001b[39m state\n\u001b[0;32m--> 452\u001b[0m   m \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m b1) \u001b[38;5;241m*\u001b[39m g \u001b[38;5;241m+\u001b[39m \u001b[43mb1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m  \u001b[38;5;66;03m# First  moment estimate.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m   u \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmaximum(b2 \u001b[38;5;241m*\u001b[39m u, jnp\u001b[38;5;241m.\u001b[39mabs(g))  \u001b[38;5;66;03m# Update exponentially weighted infinity norm.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m   x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m (step_size(i) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(b1, m\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))) \u001b[38;5;241m*\u001b[39m m\n\u001b[1;32m    455\u001b[0m        \u001b[38;5;241m/\u001b[39m (u \u001b[38;5;241m+\u001b[39m eps))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:258\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    256\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 258\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _rejected_binop_types):\n\u001b[1;32m    260\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported operand type(s) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopchar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(args[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(args[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " GAN().train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08557d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
