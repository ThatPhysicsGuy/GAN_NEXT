{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8564726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os, sys\n",
    "import pathlib\n",
    "import jax\n",
    "import pandas as pd\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jax.example_libraries import stax\n",
    "from jax import grad, jit, vmap\n",
    "import time\n",
    "\n",
    "dir_dir = '/Users/mxd6118/Desktop/DiffSim/'\n",
    "#src_dir = os.path.dirname(dir_dir) + \"/src/\"\n",
    "sys.path.insert(0,'/Users/mxd6118/Desktop/DiffSim')\n",
    "from Plots import *\n",
    "\n",
    "from src.simulator.NEW_Simulator_normal import simulate_waveforms, init_params\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a73149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, name_data):\n",
    "        \n",
    "        with open(f'{name_data}.pickle','wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "            f.close()\n",
    "\n",
    "class Producer():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.key = random.PRNGKey(int(time.time()))\n",
    "        number_of_events = int(input('How many events to be produced?'))\n",
    "        params_path = dir_dir + \"/bin/output/8530/krypton/test_noise_constant_uniform/trained_params.pickle\"\n",
    "        params = self.load_state(params_path)\n",
    "        \n",
    "        \n",
    "        self.dataloader  = self.build_dataloader(number_of_events)\n",
    "        \n",
    "        \n",
    "        batch_real = next(self.dataloader.iterate())\n",
    "        batch_real['Label'] = np.ones(len( batch_real['energy_deposits']))\n",
    "        \n",
    "        energy_depo = batch_real['energy_deposits']#self.build_random_batch(number_of_events)\n",
    "        \n",
    "        start = time.time()\n",
    "        produced_pmt,produced_sipm = self.arrays(energy_depo,params)\n",
    "        time_taken = time.time() - start\n",
    "        \n",
    "        batch_fake = {\"energy_deposits\": energy_depo,\n",
    "                 \"PMT_FAKE\": np.array(produced_pmt),\n",
    "                 \"SIPM_FAKE\": np.array(produced_sipm),\n",
    "                 \"Label_tempo\": np.zeros(len(energy_depo))}\n",
    "        \n",
    "        self.data_set = batch_real | batch_fake\n",
    "        \n",
    "        #get_data(data_set,f'batch_produced_{number_of_events}')\n",
    "        \n",
    "        print(f'All done, time taken {time_taken} sec')\n",
    "\n",
    "    def build_dataloader(self,number_of_events):\n",
    "\n",
    "        from src.utils.dataloaders.krypton_DATES_CUSTOM_DROPOUT import krypton\n",
    "        # Load the sipm database:\n",
    "        sipm_db = pd.read_pickle(\"/Users/mxd6118/Desktop/DiffSim/database/new_sipm.pkl\")\n",
    "    \n",
    "        dl = krypton(\n",
    "            batch_size  = number_of_events,\n",
    "            db          = sipm_db,\n",
    "            path        = \"/Users/mxd6118/Desktop/DiffSim/kdst\",\n",
    "            run         = 8530,\n",
    "            shuffle = True,\n",
    "            drop = 0,\n",
    "            z_slice = 0,\n",
    "            )\n",
    "            \n",
    "        return dl\n",
    "\n",
    "    \n",
    "    def arrays(self,monitor_data,params):\n",
    "        \n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        # First, run the monitor data through the simulator:\n",
    "        simulated_pmts, simulated_sipms = simulate_waveforms(monitor_data, params, subkey)\n",
    "        \n",
    "        return simulated_pmts, simulated_sipms\n",
    "        \n",
    "    def load_state(self,file):\n",
    "        with open(file,\"rb\") as f:\n",
    "            params = pickle.load(f)\n",
    "        return params\n",
    "        \n",
    "\n",
    "    def build_random_batch(self, number_of_events):\n",
    "    \n",
    "        batch =[]\n",
    "        for i in range(0,number_of_events):\n",
    "            one = np.hstack((np.random.uniform(low = -150, high = 150),\n",
    "                             np.random.uniform(low = -150, high = 150),\n",
    "                             np.random.uniform(low = 20,   high = 500),0.0415575))\n",
    "\n",
    "            two = np.vstack((one,np.zeros(4)))\n",
    "    \n",
    "            batch.append(two)\n",
    "\n",
    "        return np.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adc42d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.example_libraries import optimizers as jax_opt\n",
    "\n",
    "class GAN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        prod = Producer()\n",
    "        \n",
    "        data = prod.data_set\n",
    "        \n",
    "        self.batch = self.Chanteclair(data)\n",
    "        \n",
    "        print(self.batch['Train'].shape,flush = True)\n",
    "        \n",
    "        self.key = random.PRNGKey(int(time.time()))\n",
    "        \n",
    "        self.key, self.subkey = random.split(self.key)\n",
    "        \n",
    "        parameters, self.dis_apply = self.init_params(self.subkey)\n",
    "        \n",
    "        #print(self.out_size, flush = True)\n",
    "        \n",
    "        self.trainer = self.build_trainer(self.batch, self.dis_apply, parameters)\n",
    "        \n",
    "       \n",
    "    def Chanteclair(self,data):\n",
    "        train_batch_filtered = {}\n",
    "        train_batch_filtered['S2Si'] = []\n",
    "        train_batch_filtered['SIPM_FAKE'] =[]\n",
    "\n",
    "\n",
    "        for n in range(0,len(data['energy_deposits'])):\n",
    "            train_batch_filtered['S2Si'].append(data['S2Si'][n])\n",
    "            train_batch_filtered['SIPM_FAKE'].append(data['SIPM_FAKE'][n])\n",
    "\n",
    "\n",
    "        l = len(train_batch_filtered['S2Si'])\n",
    "\n",
    "        train_batch_filtered['train'] = np.vstack((train_batch_filtered['S2Si'],\n",
    "                                                   train_batch_filtered['SIPM_FAKE']))\n",
    "        \n",
    "        labels =[]\n",
    "\n",
    "        for c in range(0,2*l):\n",
    "            if c < l:\n",
    "                labels.append(np.array((1,0)))\n",
    "            else:\n",
    "                labels.append(np.array((0,1)))\n",
    "\n",
    "        train_batch_filtered['Labels'] = np.array(labels)\n",
    "\n",
    "\n",
    "        train, labels = sklearn.utils.shuffle(train_batch_filtered['train'],\n",
    "                                              train_batch_filtered['Labels'])\n",
    "\n",
    "\n",
    "        batch = {'Train': train, 'Labels' :labels}\n",
    "        \n",
    "        return batch \n",
    "        \n",
    "    \n",
    "    def init_params(self, subkey):\n",
    "        \n",
    "        dis_init, dis_apply = stax.serial(\n",
    "            stax.Flatten,\n",
    "            stax.Dense(128),stax.Sigmoid,\n",
    "            stax.Dense(16), stax.Sigmoid,\n",
    "            stax.Dense(2),stax.Softmax\n",
    "        )\n",
    "        \n",
    "        dis_out_size, dis_network_params = dis_init(subkey,(1,47,47,550))\n",
    "        \n",
    "        parameters = {\n",
    "        'D_parameters': dis_network_params\n",
    "        }\n",
    "        \n",
    "        return parameters, dis_apply\n",
    "    \n",
    "    def build_trainer(self, batch, fn, params):\n",
    "\n",
    "        # Shouldn't reach this portion unless training.\n",
    "        trainer = GAN_trainer(batch, fn, params)\n",
    "        \n",
    "        return trainer\n",
    "\n",
    "   \n",
    "    def train(self):\n",
    "        c = 0\n",
    "        self.key = jax.random.PRNGKey(int(time.time()))\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "\n",
    "        while c <= 100:\n",
    "\n",
    "            metrics = {}\n",
    "            start = time.time()\n",
    "\n",
    "            metrics[\"io_time\"] = time.time() - start\n",
    "\n",
    "            train_metrics, opt_state, acc = self.trainer.train_iteration(self.batch, c)\n",
    "            \n",
    "            #print('train metrics',train_metrics.keys(),flush = True)\n",
    "\n",
    "            # print(model_parameters.keys())\n",
    "            # print(model_parameters['diffusion'])\n",
    "            \n",
    "            metrics.update(train_metrics)\n",
    "\n",
    "            metrics['time'] = time.time() - start\n",
    "            metrics['accuracy'] = acc\n",
    "        \n",
    "\n",
    "            if c % 1 == 0:\n",
    "                print(f\"step = {c}, loss = {metrics['loss/loss']:.3f}, acc = {metrics['accuracy']:.3f}, time = {metrics['time']:.3f}\",flush = True)\n",
    "\n",
    "            c += 1\n",
    "            \n",
    "        #get_data(self.trainer.get_params(opt_state),'D_params')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eb37d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-8 # Small value to avoid division by zero\n",
    "    y_pred = jnp.clip(y_pred, epsilon, 1.0 - epsilon)  # Clip values to prevent NaNs\n",
    "    loss = -(y_true * jnp.log(y_pred) + (1 - y_true) * jnp.log(1 - y_pred))\n",
    "    return jnp.mean(loss)\n",
    "    \n",
    "\n",
    "class GAN_trainer():\n",
    "    \n",
    "    def __init__(self,batch,fn,parameters):\n",
    "        \n",
    "        self.key = jax.random.PRNGKey(int(time.time()))\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        self.dis_apply = fn\n",
    "        \n",
    "        @jit\n",
    "        def forward_pass(batch, parameters, key):\n",
    "\n",
    "            fake_labels = self.dis_apply(parameters['D_parameters'], batch['Train'])\n",
    "        \n",
    "            loss = binary_cross_entropy(batch['Labels'],fake_labels)\n",
    "                          \n",
    "            return loss\n",
    "        \n",
    "        self.gradient_fn = jit(jax.value_and_grad(forward_pass, argnums=1))\n",
    "        \n",
    "        opt_init, opt_update, get_params = jax_opt.adamax(2e-3)\n",
    "            \n",
    "            \n",
    "        self.opt_state = opt_init(parameters)\n",
    "\n",
    "        self.opt_update = opt_update\n",
    "        self.get_params = get_params\n",
    "    \n",
    "    def parameters(self):\n",
    "        \n",
    "        p = self.get_params(self.opt_state)\n",
    "        \n",
    "        #print(p.keys())\n",
    "    \n",
    "        parameters =  p['D_parameters']\n",
    "        \n",
    "        return parameters\n",
    "        \n",
    "    \n",
    "    def train_iteration(self, batch, c):\n",
    "        \n",
    "        metrics = {}\n",
    "\n",
    "        self.parameters = self.get_params(self.opt_state)\n",
    "        \n",
    "        #print(parameters)\n",
    "\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "\n",
    "        loss, gradients = self.gradient_fn(batch, self.parameters, subkey)\n",
    "        \n",
    "        #print(gradients['Dis_parameters'])\n",
    "        \n",
    "        self.opt_state = self.opt_update(c, gradients, self.opt_state)\n",
    "\n",
    "        metrics['loss/loss'] = loss\n",
    "\n",
    "        metrics.update(self.parameters)\n",
    "        \n",
    "        accuracy = self.acc(self.parameters,self.dis_apply, batch)\n",
    "\n",
    "        return metrics, self.opt_state, accuracy\n",
    "    \n",
    "    def acc(self, parameters,fn,batch):\n",
    "        \n",
    "        fake_labels = fn(parameters['D_parameters'], batch['Train'])\n",
    "        \n",
    "        accuracy = len(np.unique(np.where(np.argmax(fake_labels, axis = 1) - np.argmax(batch['Labels'],axis=1) == 0))) /len(batch['Labels'])\n",
    "        \n",
    "        return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d3d65c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many events to be produced?100\n",
      "All done, time taken 28.240182161331177 sec\n",
      "(200, 47, 47, 550)\n",
      "step = 0, loss = 1.084, acc = 0.500, time = 1.846\n",
      "step = 1, loss = 0.702, acc = 0.500, time = 1.536\n",
      "step = 2, loss = 0.608, acc = 0.500, time = 1.567\n",
      "step = 3, loss = 0.534, acc = 0.810, time = 1.549\n",
      "step = 4, loss = 0.460, acc = 0.995, time = 1.544\n",
      "step = 5, loss = 0.395, acc = 0.995, time = 1.545\n",
      "step = 6, loss = 0.346, acc = 0.995, time = 1.543\n",
      "step = 7, loss = 0.312, acc = 0.995, time = 1.546\n",
      "step = 8, loss = 0.287, acc = 0.995, time = 1.548\n",
      "step = 9, loss = 0.268, acc = 0.995, time = 1.560\n",
      "step = 10, loss = 0.252, acc = 0.995, time = 1.543\n",
      "step = 11, loss = 0.239, acc = 0.995, time = 1.544\n",
      "step = 12, loss = 0.228, acc = 0.995, time = 1.552\n",
      "step = 13, loss = 0.218, acc = 0.995, time = 1.552\n",
      "step = 14, loss = 0.209, acc = 1.000, time = 1.550\n",
      "step = 15, loss = 0.201, acc = 1.000, time = 1.547\n",
      "step = 16, loss = 0.193, acc = 1.000, time = 1.545\n",
      "step = 17, loss = 0.186, acc = 1.000, time = 1.553\n",
      "step = 18, loss = 0.180, acc = 1.000, time = 1.552\n",
      "step = 19, loss = 0.174, acc = 1.000, time = 1.546\n",
      "step = 20, loss = 0.168, acc = 1.000, time = 1.551\n",
      "step = 21, loss = 0.163, acc = 1.000, time = 1.550\n",
      "step = 22, loss = 0.159, acc = 1.000, time = 1.547\n",
      "step = 23, loss = 0.154, acc = 1.000, time = 1.552\n",
      "step = 24, loss = 0.150, acc = 1.000, time = 1.547\n",
      "step = 25, loss = 0.147, acc = 1.000, time = 1.548\n",
      "step = 26, loss = 0.143, acc = 1.000, time = 1.550\n",
      "step = 27, loss = 0.140, acc = 1.000, time = 1.553\n",
      "step = 28, loss = 0.137, acc = 1.000, time = 1.552\n",
      "step = 29, loss = 0.134, acc = 1.000, time = 1.577\n",
      "step = 30, loss = 0.132, acc = 1.000, time = 1.555\n",
      "step = 31, loss = 0.129, acc = 1.000, time = 1.565\n",
      "step = 32, loss = 0.127, acc = 1.000, time = 1.567\n",
      "step = 33, loss = 0.125, acc = 1.000, time = 1.570\n",
      "step = 34, loss = 0.122, acc = 1.000, time = 1.556\n",
      "step = 35, loss = 0.120, acc = 1.000, time = 1.567\n",
      "step = 36, loss = 0.118, acc = 1.000, time = 1.566\n",
      "step = 37, loss = 0.117, acc = 1.000, time = 1.582\n",
      "step = 38, loss = 0.115, acc = 1.000, time = 1.636\n",
      "step = 39, loss = 0.113, acc = 1.000, time = 1.586\n",
      "step = 40, loss = 0.111, acc = 1.000, time = 1.556\n",
      "step = 41, loss = 0.110, acc = 1.000, time = 1.558\n",
      "step = 42, loss = 0.108, acc = 1.000, time = 1.575\n",
      "step = 43, loss = 0.107, acc = 1.000, time = 1.569\n",
      "step = 44, loss = 0.105, acc = 1.000, time = 1.639\n",
      "step = 45, loss = 0.104, acc = 1.000, time = 1.718\n",
      "step = 46, loss = 0.103, acc = 1.000, time = 1.603\n",
      "step = 47, loss = 0.101, acc = 1.000, time = 1.569\n",
      "step = 48, loss = 0.100, acc = 1.000, time = 1.566\n",
      "step = 49, loss = 0.099, acc = 1.000, time = 1.572\n",
      "step = 50, loss = 0.098, acc = 1.000, time = 1.584\n",
      "step = 51, loss = 0.096, acc = 1.000, time = 1.586\n",
      "step = 52, loss = 0.095, acc = 1.000, time = 1.573\n",
      "step = 53, loss = 0.094, acc = 1.000, time = 1.570\n",
      "step = 54, loss = 0.093, acc = 1.000, time = 1.562\n",
      "step = 55, loss = 0.092, acc = 1.000, time = 1.559\n",
      "step = 56, loss = 0.091, acc = 1.000, time = 1.564\n",
      "step = 57, loss = 0.090, acc = 1.000, time = 1.611\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mGAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 99\u001b[0m, in \u001b[0;36mGAN.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     97\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m---> 99\u001b[0m train_metrics, opt_state, acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m#print('train metrics',train_metrics.keys(),flush = True)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# print(model_parameters.keys())\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# print(model_parameters['diffusion'])\u001b[39;00m\n\u001b[1;32m    106\u001b[0m metrics\u001b[38;5;241m.\u001b[39mupdate(train_metrics)\n",
      "Cell \u001b[0;32mIn[14], line 66\u001b[0m, in \u001b[0;36mGAN_trainer.train_iteration\u001b[0;34m(self, batch, c)\u001b[0m\n\u001b[1;32m     62\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss/loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     64\u001b[0m metrics\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[0;32m---> 66\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdis_apply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state, accuracy\n",
      "Cell \u001b[0;32mIn[14], line 72\u001b[0m, in \u001b[0;36mGAN_trainer.acc\u001b[0;34m(self, parameters, fn, batch)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macc\u001b[39m(\u001b[38;5;28mself\u001b[39m, parameters,fn,batch):\n\u001b[0;32m---> 72\u001b[0m     fake_labels \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD_parameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39margmax(fake_labels, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels\u001b[39m\u001b[38;5;124m'\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m))) \u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/jax/example_libraries/stax.py:307\u001b[0m, in \u001b[0;36mserial.<locals>.apply_fun\u001b[0;34m(params, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m rngs \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(rng, nlayers) \u001b[38;5;28;01mif\u001b[39;00m rng \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m nlayers\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fun, param, rng \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(apply_funs, params, rngs):\n\u001b[0;32m--> 307\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/jax/example_libraries/stax.py:61\u001b[0m, in \u001b[0;36mDense.<locals>.apply_fun\u001b[0;34m(params, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_fun\u001b[39m(params, inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     60\u001b[0m   W, b \u001b[38;5;241m=\u001b[39m params\n\u001b[0;32m---> 61\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " GAN().train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24274491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
