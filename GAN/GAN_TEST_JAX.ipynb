{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8564726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os, sys\n",
    "import pathlib\n",
    "import jax\n",
    "import pandas as pd\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jax.example_libraries import stax\n",
    "from jax import grad, jit, vmap\n",
    "import time\n",
    "\n",
    "dir_dir = '/Users/mxd6118/Desktop/DiffSim/'\n",
    "#src_dir = os.path.dirname(dir_dir) + \"/src/\"\n",
    "sys.path.insert(0,'/Users/mxd6118/Desktop/DiffSim')\n",
    "from Plots import *\n",
    "\n",
    "from src.simulator.NEW_Simulator_normal import simulate_waveforms, init_params\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a73149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, name_data):\n",
    "        \n",
    "        with open(f'{name_data}.pickle','wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "            f.close()\n",
    "\n",
    "class Producer():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.key = random.PRNGKey(int(time.time()))\n",
    "        number_of_events = int(input('How many events to be produced?'))\n",
    "        params_path = dir_dir + \"/bin/output/8530/krypton/test_noise_constant_uniform/trained_params.pickle\"\n",
    "        params = self.load_state(params_path)\n",
    "        \n",
    "        \n",
    "        self.dataloader  = self.build_dataloader(number_of_events)\n",
    "        \n",
    "        \n",
    "        batch_real = next(self.dataloader.iterate())\n",
    "        batch_real['Label'] = np.ones(len( batch_real['energy_deposits']))\n",
    "        \n",
    "        energy_depo = batch_real['energy_deposits']#self.build_random_batch(number_of_events)\n",
    "        \n",
    "        start = time.time()\n",
    "        produced_pmt,produced_sipm = self.arrays(energy_depo,params)\n",
    "        time_taken = time.time() - start\n",
    "        \n",
    "        batch_fake = {\"energy_deposits\": energy_depo,\n",
    "                 \"PMT_FAKE\": np.array(produced_pmt),\n",
    "                 \"SIPM_FAKE\": np.array(produced_sipm),\n",
    "                 \"Label_tempo\": np.zeros(len(energy_depo))}\n",
    "        \n",
    "        self.data_set = batch_real | batch_fake\n",
    "        \n",
    "        #get_data(data_set,f'batch_produced_{number_of_events}')\n",
    "        \n",
    "        print(f'All done, time taken {time_taken} sec')\n",
    "\n",
    "    def build_dataloader(self,number_of_events):\n",
    "\n",
    "        from src.utils.dataloaders.krypton_DATES_CUSTOM_DROPOUT import krypton\n",
    "        # Load the sipm database:\n",
    "        sipm_db = pd.read_pickle(\"/Users/mxd6118/Desktop/DiffSim/database/new_sipm.pkl\")\n",
    "    \n",
    "        dl = krypton(\n",
    "            batch_size  = number_of_events,\n",
    "            db          = sipm_db,\n",
    "            path        = \"/Users/mxd6118/Desktop/DiffSim/kdst\",\n",
    "            run         = 8530,\n",
    "            shuffle = True,\n",
    "            drop = 0,\n",
    "            \n",
    "            z_slice = 0,\n",
    "            )\n",
    "            \n",
    "        return dl\n",
    "\n",
    "    \n",
    "    def arrays(self,monitor_data,params):\n",
    "        \n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        # First, run the monitor data through the simulator:\n",
    "        simulated_pmts, simulated_sipms = simulate_waveforms(monitor_data, params, subkey)\n",
    "        \n",
    "        return simulated_pmts, simulated_sipms\n",
    "        \n",
    "    def load_state(self,file):\n",
    "        with open(file,\"rb\") as f:\n",
    "            params = pickle.load(f)\n",
    "        return params\n",
    "        \n",
    "\n",
    "    def build_random_batch(self, number_of_events):\n",
    "    \n",
    "        batch =[]\n",
    "        for i in range(0,number_of_events):\n",
    "            one = np.hstack((np.random.uniform(low = -150, high = 150),\n",
    "                             np.random.uniform(low = -150, high = 150),\n",
    "                             np.random.uniform(low = 20,   high = 500),0.0415575))\n",
    "\n",
    "            two = np.vstack((one,np.zeros(4)))\n",
    "    \n",
    "            batch.append(two)\n",
    "\n",
    "        return np.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc42d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.example_libraries import optimizers as jax_opt\n",
    "\n",
    "class GAN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        prod = Producer()\n",
    "        \n",
    "        data = prod.data_set\n",
    "        \n",
    "        self.batch = self.Chanteclair(data)\n",
    "        \n",
    "        #print(self.batch['Labels'].shape, flush = True)\n",
    "        \n",
    "        self.key = random.PRNGKey(int(time.time()))\n",
    "        \n",
    "        self.key, self.subkey = random.split(self.key)\n",
    "        \n",
    "        parameters, self.dis_apply = self.init_params(self.subkey)\n",
    "        \n",
    "        #print(self.out_size, flush = True)\n",
    "        \n",
    "        self.trainer = self.build_trainer(self.batch, self.dis_apply, parameters)\n",
    "        \n",
    "       \n",
    "    def Chanteclair(self,data):\n",
    "        train_batch_filtered = {}\n",
    "        train_batch_filtered['S2Si'] = []\n",
    "        train_batch_filtered['SIPM_FAKE'] =[]\n",
    "\n",
    "\n",
    "        for n in range(0,len(data['energy_deposits'])):\n",
    "            for z in np.unique(np.where(data['S2Si'][n] != 0)[2]):\n",
    "\n",
    "                train_batch_filtered['S2Si'].append(data['S2Si'][n,:,:,z])\n",
    "                train_batch_filtered['SIPM_FAKE'].append(data['SIPM_FAKE'][n,:,:,z])\n",
    "\n",
    "\n",
    "        l = len(train_batch_filtered['S2Si'])\n",
    "\n",
    "        train_batch_filtered['train'] = np.vstack((train_batch_filtered['S2Si'],\n",
    "                                                   train_batch_filtered['SIPM_FAKE']))\n",
    "        \n",
    "        labels =[]\n",
    "\n",
    "        for c in range(0,2*l):\n",
    "            if c < l:\n",
    "                labels.append(np.array((1,0)))\n",
    "            else:\n",
    "                labels.append(np.array((0,1)))\n",
    "\n",
    "        train_batch_filtered['Labels'] = np.array(labels)\n",
    "\n",
    "\n",
    "        train, labels = sklearn.utils.shuffle(train_batch_filtered['train'],\n",
    "                                              train_batch_filtered['Labels'])\n",
    "\n",
    "\n",
    "        batch = {'Train': train, 'Labels' :labels}\n",
    "        \n",
    "        return batch \n",
    "        \n",
    "    \n",
    "    def init_params(self, subkey):\n",
    "        \n",
    "        dis_init, dis_apply = stax.serial(\n",
    "            stax.Flatten,\n",
    "            stax.Dense(128),stax.Sigmoid,\n",
    "            stax.Dense(16), stax.Sigmoid,\n",
    "            stax.Dense(2),stax.Softmax\n",
    "        )\n",
    "        \n",
    "        dis_out_size, dis_network_params = dis_init(subkey,(1,47,47))\n",
    "        \n",
    "        parameters = {\n",
    "        'D_parameters': dis_network_params\n",
    "        }\n",
    "        \n",
    "        return parameters, dis_apply\n",
    "    \n",
    "    def build_trainer(self, batch, fn, params):\n",
    "\n",
    "        # Shouldn't reach this portion unless training.\n",
    "        trainer = GAN_trainer(batch, fn, params)\n",
    "        \n",
    "        return trainer\n",
    "\n",
    "   \n",
    "    def train(self):\n",
    "        c = 0\n",
    "        self.key = jax.random.PRNGKey(int(time.time()))\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "\n",
    "        while c <= 100:\n",
    "\n",
    "            metrics = {}\n",
    "            start = time.time()\n",
    "\n",
    "            metrics[\"io_time\"] = time.time() - start\n",
    "\n",
    "            train_metrics, opt_state, acc = self.trainer.train_iteration(self.batch, c)\n",
    "            \n",
    "            #print('train metrics',train_metrics.keys(),flush = True)\n",
    "\n",
    "            # print(model_parameters.keys())\n",
    "            # print(model_parameters['diffusion'])\n",
    "            \n",
    "            metrics.update(train_metrics)\n",
    "\n",
    "            metrics['time'] = time.time() - start\n",
    "            metrics['accuracy'] = acc\n",
    "        \n",
    "\n",
    "            if c % 1 == 0:\n",
    "                print(f\"step = {c}, loss = {metrics['loss/loss']:.3f}, acc = {metrics['accuracy']:.3f}, time = {metrics['time']:.3f}\",flush = True)\n",
    "\n",
    "            c += 1\n",
    "            \n",
    "        get_data(self.trainer.get_params(opt_state),'D_params')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb37d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-8 # Small value to avoid division by zero\n",
    "    y_pred = jnp.clip(y_pred, epsilon, 1.0 - epsilon)  # Clip values to prevent NaNs\n",
    "    loss = -(y_true * jnp.log(y_pred) + (1 - y_true) * jnp.log(1 - y_pred))\n",
    "    return jnp.mean(loss)\n",
    "    \n",
    "\n",
    "class GAN_trainer():\n",
    "    \n",
    "    def __init__(self,batch,fn,parameters):\n",
    "        \n",
    "        self.key = jax.random.PRNGKey(int(time.time()))\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        self.dis_apply = fn\n",
    "        @jit\n",
    "        def forward_pass(batch, parameters, key):\n",
    "            \n",
    "\n",
    "            fake_labels = self.dis_apply(parameters['D_parameters'], batch['Train'])\n",
    "        \n",
    "            loss = binary_cross_entropy(batch['Labels'],fake_labels)\n",
    "                          \n",
    "            return loss\n",
    "        \n",
    "        self.gradient_fn = jit(jax.value_and_grad(forward_pass, argnums=1))\n",
    "        \n",
    "        opt_init, opt_update, get_params = jax_opt.adamax(2e-3)\n",
    "            \n",
    "            \n",
    "        self.opt_state = opt_init(parameters)\n",
    "\n",
    "        self.opt_update = opt_update\n",
    "        self.get_params = get_params\n",
    "    \n",
    "    def parameters(self):\n",
    "        \n",
    "        p = self.get_params(self.opt_state)\n",
    "        \n",
    "        #print(p.keys())\n",
    "    \n",
    "        parameters =  p['D_parameters']\n",
    "        \n",
    "        return parameters\n",
    "        \n",
    "    \n",
    "    def train_iteration(self, batch, c):\n",
    "        \n",
    "        metrics = {}\n",
    "\n",
    "        self.parameters = self.get_params(self.opt_state)\n",
    "        \n",
    "        #print(parameters)\n",
    "\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "\n",
    "        loss, gradients = self.gradient_fn(batch, self.parameters, subkey)\n",
    "        \n",
    "        #print(gradients['Dis_parameters'])\n",
    "        \n",
    "        self.opt_state = self.opt_update(c, gradients, self.opt_state)\n",
    "\n",
    "        metrics['loss/loss'] = loss\n",
    "\n",
    "        metrics.update(self.parameters)\n",
    "        \n",
    "        accuracy = self.acc(self.parameters,self.dis_apply, batch)\n",
    "\n",
    "        return metrics, self.opt_state, accuracy\n",
    "    \n",
    "    def acc(self, parameters,fn,batch):\n",
    "        \n",
    "        fake_labels = fn(parameters['D_parameters'], batch['Train'])\n",
    "        \n",
    "        accuracy = len(np.unique(np.where(np.argmax(fake_labels, axis = 1) - np.argmax(batch['Labels'],axis=1) == 0))) /len(batch['Labels'])\n",
    "        \n",
    "        return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d3d65c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many events to be produced?20\n",
      "All done, time taken 2.5022847652435303 sec\n",
      "step = 0, loss = 0.713, acc = 0.500, time = 0.222\n",
      "step = 1, loss = 0.671, acc = 0.604, time = 0.008\n",
      "step = 2, loss = 0.647, acc = 0.766, time = 0.007\n",
      "step = 3, loss = 0.628, acc = 0.746, time = 0.008\n",
      "step = 4, loss = 0.609, acc = 0.768, time = 0.008\n",
      "step = 5, loss = 0.591, acc = 0.802, time = 0.009\n",
      "step = 6, loss = 0.573, acc = 0.812, time = 0.009\n",
      "step = 7, loss = 0.556, acc = 0.826, time = 0.009\n",
      "step = 8, loss = 0.540, acc = 0.841, time = 0.009\n",
      "step = 9, loss = 0.525, acc = 0.845, time = 0.008\n",
      "step = 10, loss = 0.510, acc = 0.853, time = 0.009\n",
      "step = 11, loss = 0.496, acc = 0.857, time = 0.008\n",
      "step = 12, loss = 0.481, acc = 0.865, time = 0.010\n",
      "step = 13, loss = 0.466, acc = 0.867, time = 0.010\n",
      "step = 14, loss = 0.452, acc = 0.867, time = 0.008\n",
      "step = 15, loss = 0.439, acc = 0.867, time = 0.009\n",
      "step = 16, loss = 0.426, acc = 0.867, time = 0.009\n",
      "step = 17, loss = 0.413, acc = 0.870, time = 0.009\n",
      "step = 18, loss = 0.401, acc = 0.874, time = 0.008\n",
      "step = 19, loss = 0.389, acc = 0.882, time = 0.008\n",
      "step = 20, loss = 0.378, acc = 0.886, time = 0.008\n",
      "step = 21, loss = 0.368, acc = 0.891, time = 0.009\n",
      "step = 22, loss = 0.357, acc = 0.894, time = 0.008\n",
      "step = 23, loss = 0.347, acc = 0.901, time = 0.009\n",
      "step = 24, loss = 0.338, acc = 0.906, time = 0.009\n",
      "step = 25, loss = 0.328, acc = 0.911, time = 0.008\n",
      "step = 26, loss = 0.320, acc = 0.911, time = 0.009\n",
      "step = 27, loss = 0.311, acc = 0.918, time = 0.008\n",
      "step = 28, loss = 0.304, acc = 0.923, time = 0.008\n",
      "step = 29, loss = 0.296, acc = 0.928, time = 0.008\n",
      "step = 30, loss = 0.289, acc = 0.932, time = 0.008\n",
      "step = 31, loss = 0.282, acc = 0.940, time = 0.008\n",
      "step = 32, loss = 0.275, acc = 0.940, time = 0.008\n",
      "step = 33, loss = 0.269, acc = 0.942, time = 0.008\n",
      "step = 34, loss = 0.263, acc = 0.940, time = 0.009\n",
      "step = 35, loss = 0.258, acc = 0.942, time = 0.008\n",
      "step = 36, loss = 0.252, acc = 0.942, time = 0.009\n",
      "step = 37, loss = 0.247, acc = 0.942, time = 0.008\n",
      "step = 38, loss = 0.242, acc = 0.942, time = 0.009\n",
      "step = 39, loss = 0.238, acc = 0.944, time = 0.009\n",
      "step = 40, loss = 0.233, acc = 0.947, time = 0.008\n",
      "step = 41, loss = 0.229, acc = 0.947, time = 0.007\n",
      "step = 42, loss = 0.225, acc = 0.947, time = 0.009\n",
      "step = 43, loss = 0.221, acc = 0.947, time = 0.008\n",
      "step = 44, loss = 0.217, acc = 0.947, time = 0.008\n",
      "step = 45, loss = 0.214, acc = 0.947, time = 0.008\n",
      "step = 46, loss = 0.210, acc = 0.947, time = 0.009\n",
      "step = 47, loss = 0.207, acc = 0.947, time = 0.008\n",
      "step = 48, loss = 0.203, acc = 0.944, time = 0.008\n",
      "step = 49, loss = 0.200, acc = 0.944, time = 0.008\n",
      "step = 50, loss = 0.197, acc = 0.944, time = 0.008\n",
      "step = 51, loss = 0.194, acc = 0.944, time = 0.008\n",
      "step = 52, loss = 0.192, acc = 0.944, time = 0.008\n",
      "step = 53, loss = 0.189, acc = 0.944, time = 0.007\n",
      "step = 54, loss = 0.186, acc = 0.944, time = 0.008\n",
      "step = 55, loss = 0.184, acc = 0.944, time = 0.008\n",
      "step = 56, loss = 0.181, acc = 0.944, time = 0.008\n",
      "step = 57, loss = 0.179, acc = 0.944, time = 0.008\n",
      "step = 58, loss = 0.177, acc = 0.947, time = 0.008\n",
      "step = 59, loss = 0.175, acc = 0.947, time = 0.008\n",
      "step = 60, loss = 0.172, acc = 0.947, time = 0.007\n",
      "step = 61, loss = 0.170, acc = 0.947, time = 0.007\n",
      "step = 62, loss = 0.168, acc = 0.947, time = 0.008\n",
      "step = 63, loss = 0.166, acc = 0.949, time = 0.007\n",
      "step = 64, loss = 0.164, acc = 0.954, time = 0.008\n",
      "step = 65, loss = 0.163, acc = 0.954, time = 0.008\n",
      "step = 66, loss = 0.161, acc = 0.952, time = 0.008\n",
      "step = 67, loss = 0.159, acc = 0.954, time = 0.008\n",
      "step = 68, loss = 0.157, acc = 0.954, time = 0.008\n",
      "step = 69, loss = 0.155, acc = 0.954, time = 0.008\n",
      "step = 70, loss = 0.154, acc = 0.954, time = 0.007\n",
      "step = 71, loss = 0.152, acc = 0.954, time = 0.008\n",
      "step = 72, loss = 0.151, acc = 0.954, time = 0.008\n",
      "step = 73, loss = 0.149, acc = 0.954, time = 0.008\n",
      "step = 74, loss = 0.148, acc = 0.954, time = 0.008\n",
      "step = 75, loss = 0.146, acc = 0.954, time = 0.007\n",
      "step = 76, loss = 0.145, acc = 0.954, time = 0.008\n",
      "step = 77, loss = 0.143, acc = 0.954, time = 0.008\n",
      "step = 78, loss = 0.142, acc = 0.954, time = 0.008\n",
      "step = 79, loss = 0.141, acc = 0.954, time = 0.008\n",
      "step = 80, loss = 0.139, acc = 0.954, time = 0.008\n",
      "step = 81, loss = 0.138, acc = 0.957, time = 0.008\n",
      "step = 82, loss = 0.137, acc = 0.957, time = 0.008\n",
      "step = 83, loss = 0.135, acc = 0.957, time = 0.008\n",
      "step = 84, loss = 0.134, acc = 0.957, time = 0.008\n",
      "step = 85, loss = 0.133, acc = 0.957, time = 0.007\n",
      "step = 86, loss = 0.132, acc = 0.957, time = 0.008\n",
      "step = 87, loss = 0.131, acc = 0.957, time = 0.008\n",
      "step = 88, loss = 0.130, acc = 0.959, time = 0.009\n",
      "step = 89, loss = 0.128, acc = 0.959, time = 0.008\n",
      "step = 90, loss = 0.127, acc = 0.959, time = 0.007\n",
      "step = 91, loss = 0.126, acc = 0.959, time = 0.007\n",
      "step = 92, loss = 0.125, acc = 0.959, time = 0.008\n",
      "step = 93, loss = 0.124, acc = 0.961, time = 0.008\n",
      "step = 94, loss = 0.123, acc = 0.961, time = 0.008\n",
      "step = 95, loss = 0.122, acc = 0.961, time = 0.008\n",
      "step = 96, loss = 0.121, acc = 0.961, time = 0.008\n",
      "step = 97, loss = 0.120, acc = 0.959, time = 0.007\n",
      "step = 98, loss = 0.119, acc = 0.961, time = 0.007\n",
      "step = 99, loss = 0.118, acc = 0.964, time = 0.008\n",
      "step = 100, loss = 0.117, acc = 0.964, time = 0.008\n"
     ]
    }
   ],
   "source": [
    " GAN().train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062698b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
